{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JJThoms1/04_transfer_learning_in_tensorflow_part_1_feature_extraction/blob/main/04_transfer_learning_in_tensorflow_part_1_feature_extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUCqWGHhTPe6"
      },
      "source": [
        "# Transfer Learning with Tensorflow Part 1: Feature Extraction\n",
        "\n",
        "Transfer learning is leveraging a working model's existing architecture and learning patterns for our own problems.\n",
        "\n",
        "There are two main benefits:\n",
        "1. Can leverage an axisting neural network architecture proven to work on problems similar to our own.\n",
        "2. Can leverage a working neural network architecture which has already learned patterns on similar data to our own, then we can adapt those patterns to our own data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-gL1ekXU7LL",
        "outputId": "ae629f26-14cb-4466-f31c-7f054c6a4d21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Oct  2 15:08:30 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P8              11W /  70W |      3MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Are we using a GPU?\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbbkkUCEVj49",
        "outputId": "4656d850-fe25-4695-daee-f2d1f830d4e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-10-02 15:08:31--  https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.101.207, 142.251.2.207, 142.250.141.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.101.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 168546183 (161M) [application/zip]\n",
            "Saving to: â€˜10_food_classes_10_percent.zip.3â€™\n",
            "\n",
            "10_food_classes_10_ 100%[===================>] 160.74M   141MB/s    in 1.1s    \n",
            "\n",
            "2024-10-02 15:08:32 (141 MB/s) - â€˜10_food_classes_10_percent.zip.3â€™ saved [168546183/168546183]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Get data (10% of 10 food classes from Food101) - https://www.kaggle.com/datasets/dansbecker/food-101\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Download the data\n",
        "!wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip\n",
        "\n",
        "# Check if the file exists\n",
        "if not os.path.exists(\"10_food_classes_10_percent.zip\"):\n",
        "    print(\"Error: File not found. Please download the file again.\")\n",
        "else:\n",
        "  # Unzip the download file\n",
        "  zip_ref = zipfile.ZipFile(\"10_food_classes_10_percent.zip\")\n",
        "  zip_ref.extractall()\n",
        "  zip_ref.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-bhvvIWwe5sA",
        "outputId": "0d43a7b0-d752-494e-ff35-f8e030cbeebf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 2 directories and 0 images in '10_food_classes_10_percent'.\n",
            "There are 10 directories and 0 images in '10_food_classes_10_percent/test'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/sushi'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/ice_cream'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/fried_rice'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/hamburger'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/grilled_salmon'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/ramen'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/chicken_wings'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/chicken_curry'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/pizza'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/steak'.\n",
            "There are 10 directories and 0 images in '10_food_classes_10_percent/train'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/sushi'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/ice_cream'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/fried_rice'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/hamburger'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/grilled_salmon'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/ramen'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/chicken_wings'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/chicken_curry'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/pizza'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/steak'.\n"
          ]
        }
      ],
      "source": [
        "# How many images in each folder\n",
        "import os\n",
        "\n",
        "# Walk through 10 percent of data directory and list number of files\n",
        "for dirpath, dirnames, filenames in os.walk(\"10_food_classes_10_percent\"):\n",
        "  print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fwR6YYNuiur"
      },
      "source": [
        "## Creating data loaders (preparing the data)\n",
        "\n",
        "We'll use the `ImageDataGenerator` class to load in our images in batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjIDUOxYvwi-",
        "outputId": "a3c69c2d-1b41-4f3d-d786-0cb460166f29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training images:\n",
            "Found 750 images belonging to 10 classes.\n",
            "Testing images:\n",
            "Found 2500 images belonging to 10 classes.\n"
          ]
        }
      ],
      "source": [
        "# Setup data inputs\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "IMAGE_SHAPE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_dir = \"10_food_classes_10_percent/train/\"\n",
        "test_dir = \"10_food_classes_10_percent/test/\"\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1/255.0)\n",
        "test_datagen = ImageDataGenerator(rescale=1/255.0)\n",
        "\n",
        "print(\"Training images:\")\n",
        "train_data_10_percent = train_datagen.flow_from_directory(train_dir,\n",
        "                                               target_size=IMAGE_SHAPE,\n",
        "                                               batch_size=BATCH_SIZE,\n",
        "                                               class_mode=\"categorical\")\n",
        "\n",
        "print(\"Testing images:\")\n",
        "test_data = test_datagen.flow_from_directory(test_dir,\n",
        "                                             target_size=IMAGE_SHAPE,\n",
        "                                             batch_size=BATCH_SIZE,\n",
        "                                             class_mode=\"categorical\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHwbEISgwpfj"
      },
      "source": [
        "## Setting up callbacks (things to run whilst our model trains)\n",
        "\n",
        "Callbacks are extra functionality you can add to your models to be performed during and after training. Some of the most popular callbacks are:\n",
        "\n",
        "* Tracking experiments with the TensorBoard callbacks\n",
        "* Model checkpoint with ModelCheckpoint callback\n",
        "* Stopping a model from training (before it trains too long and overfits) with the EarlyStopping callback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "R_6HZikGynci"
      },
      "outputs": [],
      "source": [
        "# Create TensorBoard callback (functionalized because we need to create a new one for each model)\n",
        "import datetime\n",
        "\n",
        "def create_tensorboard_callback(dir_name, experiment_name):\n",
        "  log_dir = dir_name + \"/\" + experiment_name + \"/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "  tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
        "  print(f\"Saving TensorBoard log files to: {log_dir}\")\n",
        "  return tensorboard_callback"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvvyNpgRFVT8"
      },
      "source": [
        "ðŸ”‘ **Note:** You can customize the directory where your TensorBoard logs(model training metrics) get saved to wherever you like. The log_dir parameter we've created above is only one option."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "_NLJ4S0PGJ-K"
      },
      "outputs": [],
      "source": [
        "# Let's compare the following two models\n",
        "resnet_url = \"https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/4\"\n",
        "\n",
        "efficientnet_url = \"https://tfhub.dev/google/efficientnet/b0/feature-vector/1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pn8eZok11eff"
      },
      "source": [
        "## Creating models using TensorFlow Hub\n",
        "\n",
        "In the past we've used Tensorflow to create our own models layer by layer from scratch.\n",
        "\n",
        "Now we're going to do a similar process, except the majority of our model's layers are going to come from TensorFlow Hub.\n",
        "\n",
        "We can access pretrained model on: https://tfhub.dev/\n",
        "\n",
        "Browsing the TensorFlow Hub page and sorting for image classification, we found the following feature vector model link: https://www.kaggle.com/models/tensorflow/efficientnet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.11.0\n",
        "!pip install tensorflow-hub==0.12.0\n",
        "!pip install tf_keras==2.11.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IcQzOaG0Dy8V",
        "outputId": "555f5137-e9b8-41be-fae3-d5e305a72129"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow==2.11.0 in /usr/local/lib/python3.10/dist-packages (2.11.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (24.3.25)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (1.64.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (3.11.0)\n",
            "Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (2.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (18.1.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (24.1)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (3.19.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (2.11.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (2.11.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (1.16.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.11.0) (0.44.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.0.4)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.2.2)\n",
            "Requirement already satisfied: tensorflow-hub==0.12.0 in /usr/local/lib/python3.10/dist-packages (0.12.0)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-hub==0.12.0) (1.26.4)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-hub==0.12.0) (3.19.6)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement tf_keras==2.11.0 (from versions: 2.14.1, 2.15.0rc0, 2.15.0rc1, 2.15.0, 2.15.1rc0, 2.15.1, 2.16.0rc0, 2.16.0rc1, 2.16.0rc2, 2.16.0rc3, 2.16.0rc4, 2.16.0, 2.17.0rc0, 2.17.0, 2.18.0rc0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for tf_keras==2.11.0\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "Raw_S3e7gxdo"
      },
      "outputs": [],
      "source": [
        "# Import dependencies\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "1znOhN-OhPAK"
      },
      "outputs": [],
      "source": [
        "# Let's make our create_model() function to create a model from a URL\n",
        "def create_model(model_url, num_classes=10, input_shape=(224, 224)):\n",
        "  \"\"\"\n",
        "  Takes a TensorFlow Hub URL and creates a Keras Sequential model with it.\n",
        "\n",
        "  Args:\n",
        "    model_url (str): A TensorFlow Hub feature extraction URL.\n",
        "    num_classes (int): Number of output neurons in the output layer,\n",
        "      should be equal to number of target classes, default 10.\n",
        "\n",
        "  Returns:\n",
        "    An uncompiled Keras Sequential model with model_url as feature\n",
        "    extractor layer and Dense output layer with num_classes outputs neurons.\n",
        "  \"\"\"\n",
        "  IMAGE_SHAPE = input_shape\n",
        "  # Download the pretrained model and save it as a Keras layer\n",
        "  feature_extractor_layer = hub.KerasLayer(model_url,\n",
        "                                           trainable=False, # freeze the already learned patterns\n",
        "                                           name=\"feature_extraction_layer\",\n",
        "                                           input_shape=IMAGE_SHAPE+(3,)) # define the input image shape\n",
        "\n",
        "  # Create the model\n",
        "  model = tf.keras.Sequential([\n",
        "     feature_extractor_layer, # Add the feature extraction layer\n",
        "     layers.Dense(num_classes, activation=\"softmax\", name=\"output_layer\") # Add the output layer\n",
        "  ])\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eK3b9RNjDqT"
      },
      "source": [
        "### Creating and testing ResNet TensorFlow Hub feature Exraction model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "xhQxiqheY4ZZ"
      },
      "outputs": [],
      "source": [
        "resnet_model = create_model(resnet_url,\n",
        "                            num_classes=train_data_10_percent.num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "hYeOOm3DYSRB"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "resnet_model.compile(loss=\"categorical_crossentropy\",\n",
        "                     optimizer=tf.keras.optimizers.Adam(),\n",
        "                     metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resnet_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zzApnvtPeoO",
        "outputId": "bf8bc5bc-e24b-49a6-c9d0-8f723011a7e4"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " feature_extraction_layer (K  (None, 2048)             23564800  \n",
            " erasLayer)                                                      \n",
            "                                                                 \n",
            " output_layer (Dense)        (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 23,585,290\n",
            "Trainable params: 20,490\n",
            "Non-trainable params: 23,564,800\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pM8I0EKnYjy7",
        "outputId": "ff25492f-e2d5-42cb-d553-a5a28b40e515"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving TensorBoard log files to: tensorflow_hub/resnet50V2/20241002-150844\n",
            "Epoch 1/5\n",
            "24/24 [==============================] - 339s 14s/step - loss: 1.8860 - accuracy: 0.3840 - val_loss: 1.2073 - val_accuracy: 0.6112\n",
            "Epoch 2/5\n",
            "24/24 [==============================] - 336s 14s/step - loss: 0.9217 - accuracy: 0.7200 - val_loss: 0.8581 - val_accuracy: 0.7192\n",
            "Epoch 3/5\n",
            "24/24 [==============================] - 326s 14s/step - loss: 0.6292 - accuracy: 0.8253 - val_loss: 0.7358 - val_accuracy: 0.7632\n",
            "Epoch 4/5\n",
            "24/24 [==============================] - 325s 14s/step - loss: 0.4812 - accuracy: 0.8760 - val_loss: 0.6989 - val_accuracy: 0.7732\n",
            "Epoch 5/5\n",
            "24/24 [==============================] - 317s 14s/step - loss: 0.3845 - accuracy: 0.9093 - val_loss: 0.6650 - val_accuracy: 0.7892\n"
          ]
        }
      ],
      "source": [
        "# Let's fit our ResNet model to the data (10% of 10 classes)\n",
        "resnet_history = resnet_model.fit(train_data_10_percent,\n",
        "                                  epochs=5,\n",
        "                                  steps_per_epoch=len(train_data_10_percent),\n",
        "                                  validation_data=test_data,\n",
        "                                  validation_steps=len(test_data),\n",
        "                                  callbacks=[create_tensorboard_callback(dir_name=\"tensorflow_hub\",\n",
        "                                                                        experiment_name=\"resnet50V2\"\n",
        "                                                                        )])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wow!\n",
        "\n",
        "That. Is. Incredible. Our transfer learning feature extractor model out preformed all of the previous models we built by hand... (substantially) and in a quicker training time. And with only 10% of the training examples."
      ],
      "metadata": {
        "id": "eW0oHudb4Io1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "BZLKRxLbXTS5"
      },
      "outputs": [],
      "source": [
        "# Let's create a function to plot our loss curves"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "mount_file_id": "1F4YYaCuKqmClkVVMaTd6YMju6H_heYej",
      "authorship_tag": "ABX9TyNOzI9LjQHEhGfVcPvy6v4O",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}